{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pf\n",
    "import os,sys,time\n",
    "import shutil\n",
    "\n",
    "class biliSpider(object):\n",
    "    #创建项目\n",
    "        \n",
    "    def createProject(self):\n",
    "        shutil.copytree('template',self.dir)  \n",
    "        os.chdir(self.dir)#改变工作目录到template\n",
    "        result=os.rename(u'$projectName$',self.projectName) #创建工程目录\n",
    "        \n",
    "        #修改scrapy.cfg 文件\n",
    "        with open('scrapy.cfg','a+') as fr:\n",
    "            tmp=fr.read()\n",
    "        tmp=tmp.replace('$projectName$',self.projectName)\n",
    "        with open('scrapy.cfg','w') as fw:\n",
    "            fw.write(tmp)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    #配置settings\n",
    "    def settings(self,mongodb_ip='192.168.11.17',mongodb_db='CrawlerData_other',mongodb_table='test'):\n",
    "        #修改settings 文件\n",
    "        os.chdir(self.projectName)#改变工作目录到template\n",
    "        with open('settings.py','a+') as fr:\n",
    "            tmp=fr.read()\n",
    "        tmp=tmp.replace('$projectName$',self.projectName)\n",
    "        tmp=tmp.replace('$mongodb_ip$',mongodb_ip)\n",
    "        tmp=tmp.replace('$mongodb_db$',mongodb_db)\n",
    "        tmp=tmp.replace('$mongodb_table$',mongodb_table)\n",
    "        with open('settings.py','w') as fw:\n",
    "            fw.write(tmp)\n",
    "            fw.close()\n",
    "    #创建items\n",
    "    def createItems(self,items):\n",
    "       \n",
    "        items_txt=''\n",
    "        for item in items:\n",
    "            items_txt=items_txt+item+'=scrapy.Field()\\n+    '\n",
    "        with open('items.py','a+') as fr:\n",
    "            tmp=fr.read()\n",
    "        tmp=tmp.replace('$items$',items_txt.replace('+',''))\n",
    "        with open('items.py','w') as fw:\n",
    "            fw.write(tmp)\n",
    "    #创建spider\n",
    "    def createSpider(self):\n",
    "        os.chdir(self.dir+'\\\\'+self.projectName+'\\\\'+'spiders')#改变工作目录到spiders\n",
    "          #修改scrapy.cfg 文件\n",
    "        with open('Spider.py','a+') as fr:\n",
    "            tmp=fr.read()\n",
    "        tmp=tmp.replace('$projectName$',self.projectName)\n",
    "        tmp=tmp.replace('$spiderName$',self.spiderName)\n",
    "        with open('Spider.py','w') as fw:\n",
    "            fw.write(tmp)\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "    def StartReqByKw(self,url_='www.example.com/{keyword}/{page}',req_method='get',parse='self.parse_list_item'):\n",
    "        os.chdir(self.dir+'\\\\'+self.projectName+'\\\\'+'spiders')#改变工作目录到spiders\n",
    "        with open('Spider.py','a+') as fr:\n",
    "            spiders_tmp=fr.read()\n",
    "        if req_method.lower()=='get':\n",
    "            req_tmp='''\n",
    "    headers={\n",
    "            'Accept':'application/json, text/plain, */*',\n",
    "            'Content-Type':'application/x-www-form-urlencoded',\n",
    "            'Host':'www.zhaodahao.com',\n",
    "            'Referer':'http://www.zhaodahao.com/home/index/index.html',\n",
    "            'User_Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:58.0) Gecko/20100101 Firefox/58.0',\n",
    "            'X-Requested-With':'XMLHttpRequest',\n",
    "        }\n",
    "    def start_requests(self):\n",
    "        with codecs.open('keywords.csv',encoding='utf-8') as fr:\n",
    "            keywords = fr.readlines()\n",
    "        page = 1\n",
    "        for keyword in keywords:\n",
    "            keyword=keyword.replace('\\\\n','')\n",
    "            url_='$url_$'\n",
    "            url = url_.format(keyword=keyword, page=str(page))\n",
    "            item = Item()\n",
    "            item['keyword'] = keyword\n",
    "            item['date']=datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "            item['spider_name']=self.name\n",
    "            req = scrapy.Request(url, callback=$parse$)\n",
    "            req.meta['page'] = page\n",
    "            req.meta['item'] = copy.deepcopy(item)\n",
    "            req.meta['url_'] = copy.deepcopy(url_)\n",
    "            yield req\n",
    "        '''.replace('$url_$',url_).replace('$parse$',parse)\n",
    "        else:\n",
    "            req_tmp='''\n",
    "        headers={\n",
    "            'Accept':'application/json, text/plain, */*',\n",
    "            'Content-Type':'application/x-www-form-urlencoded',\n",
    "            'Host':'www.zhaodahao.com',\n",
    "            'Referer':'http://www.zhaodahao.com/home/index/index.html',\n",
    "            'User_Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:58.0) Gecko/20100101 Firefox/58.0',\n",
    "            'X-Requested-With':'XMLHttpRequest',\n",
    "        }\n",
    "\n",
    "    def start_requests(self):\n",
    "        with codecs.open('keywords.csv',encoding='utf-8') as fr:\n",
    "            keywords = fr.readlines()\n",
    "        page = 1\n",
    "        for keyword in keywords:\n",
    "            keyword=keyword.replace('\\\\n','')\n",
    "            url_='$url_$'\n",
    "            url = url_.format(keyword=keyword, page=str(page))\n",
    "            \n",
    "            item = Item()\n",
    "            item['keyword'] = keyword\n",
    "            item['date']=datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "            item['spider_name']=self.name\n",
    "            req = scrapy.FormRequest(url,formdata= post_data,callback=$parse$)\n",
    "            req.meta['page'] = page\n",
    "            req.meta['item'] = copy.deepcopy(item)\n",
    "            req.meta['url_'] = copy.deepcopy(url_)\n",
    "            yield req\n",
    "        '''.replace('$url_$',url_).replace('$parse$',parse)\n",
    "                \n",
    "        spiders_tmp=spiders_tmp.replace('$nextStep$',req_tmp)\n",
    "        spiders_tmp=spiders_tmp.replace('$url_$',url_)\n",
    "        with open('Spider.py','w') as fw:\n",
    "            fw.write(spiders_tmp)\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "\n",
    "    def ParseHtml(self,res_meta_keys,line_keys,req_meta_keys,name='',next_step_type='item',callback='',method='get',TP_page_limit=10,TP_count_limit=10,TP_method='get'):\n",
    "        os.chdir(self.dir+'\\\\'+self.projectName+'\\\\'+'spiders')#改变工作目录到spiders\n",
    "        with open('nextStep.py','a+') as f:\n",
    "            SC=f.read()\n",
    "        SC=SC.replace('$name$',name)\n",
    "        SC=SC.replace('$res_meta_keys$',self._create_res_meta(res_meta_keys))\n",
    "        pre_res='''\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        lines = soup.find(class_=\"result\").find_all(class_=\"list-dl\")\n",
    "        '''\n",
    "        SC=SC.replace('$pre_res$',pre_res)\n",
    "        if TP_page_limit<>0 and TP_count_limit<>0:\n",
    "            SC = SC.replace('$turn_page$',self._next_page(self.req_meta,method=TP_method,callback='self.'+name))\n",
    "        SC=SC.replace('$line_keys$',self._create_lines_and_items(line_keys,self.items))\n",
    "        SC = SC.replace('$to_next_step$', self._to_next_step(next_step_type,req_meta=req_meta_keys,method=method,callback=callback))\n",
    "        with open('Spider.py','a+') as f:\n",
    "            f.write(SC)\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "    def ParseJson(self,res_meta_keys,line_keys,req_meta_keys,name='',next_step_type='item',callback='',method='get',TP_page_limit=10,TP_count_limit=10,TP_method='get'):\n",
    "        os.chdir(self.dir+'\\\\'+self.projectName+'\\\\'+'spiders')#改变工作目录到spiders\n",
    "        with open('nextStep.py','a+') as f:\n",
    "            SC=f.read()\n",
    "        SC=SC.replace('$name$',name)\n",
    "        SC=SC.replace('$res_meta_keys$',self._create_res_meta(res_meta_keys))\n",
    "        pre_res='''\n",
    "        json_text=response.text\n",
    "        json_data=json.loads(json_text)\n",
    "        lines=json_data['result']\n",
    "        '''\n",
    "        SC=SC.replace('$pre_res$',pre_res)\n",
    "        if TP_page_limit<>0 and TP_count_limit<>0:\n",
    "            SC = SC.replace('$turn_page$',self._next_page(self.req_meta,method=TP_method,callback='self.'+name))\n",
    "        SC=SC.replace('$line_keys$',self._create_lines_and_items_from_json(line_keys,self.items))\n",
    "        SC = SC.replace('$to_next_step$', self._to_next_step(next_step_type,req_meta=req_meta_keys,method=method,callback=callback))\n",
    "        with open('Spider.py','a+') as f:\n",
    "            f.write(SC)\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "\n",
    "    def _create_res_meta(self,res_meta, blank_n=8):\n",
    "        script = ''\n",
    "        for key in res_meta:\n",
    "            script = script + key + \"=response.meta['%s']\\n\" % key + \" \" * blank_n\n",
    "        return script\n",
    "\n",
    "    def _create_lines_and_items(self,lines, items, blank_n=12):\n",
    "        script = ''\n",
    "        for key in lines:\n",
    "            script = script + key + \"=line.find(class_='').find(class_='').get_text()\\n\" + \" \" * blank_n\n",
    "        script = script + '\\n' + \" \" * blank_n\n",
    "        for key in items:\n",
    "            if key in lines:\n",
    "                script = script + \"item['%s']=\" % key + key + '\\n' + \" \" * blank_n\n",
    "        return script\n",
    "    def _create_lines_and_items_from_json(self,lines, items, blank_n=12):\n",
    "        script = ''\n",
    "        for key in lines:\n",
    "            script = script + key + \"=line['%s']\\n\" % key + \" \" * blank_n\n",
    "        script = script + '\\n' + \" \" * blank_n\n",
    "        for key in items:\n",
    "            if key in lines:\n",
    "                script = script + \"item['%s']=\" % key + key + '\\n' + \" \" * blank_n\n",
    "        return script\n",
    "\n",
    "    def _create_req_meta_(self,req_meta, blank_n=12):\n",
    "        script = ''\n",
    "        for key in req_meta:\n",
    "            script = script + \"req.meta['{}']={}\\n\".format(key,key) + \" \" * blank_n\n",
    "        return script\n",
    "\n",
    "    def _next_page(self,req_meta, page_limit=10, count_limit=10, method='get', callback='self.parse'):\n",
    "\n",
    "        script = '''\n",
    "        if int(page) < $page$ or int(count) < $count$:\n",
    "            page=str(int(page+1))\n",
    "            last_url=last_url_.format(keyword=keyword, page=str(page))\n",
    "            req = $method$\n",
    "            req.meta['item'] = copy.deepcopy(item)\n",
    "            req.meta['page'] = page\n",
    "            req.meta['url_'] = copy.deepcopy(last_url_)\n",
    "            $req_meta$\n",
    "            yield req\n",
    "    '''\n",
    "        script = script.replace('$page$', str(page_limit)).replace('$count$', str(count_limit))\n",
    "        if method.lower() <> 'post':\n",
    "            method_sc = 'scrapy.Request(last_url, callback=$callback$)'.replace('$callback$', callback)\n",
    "        else:\n",
    "            method_sc = 'scrapy.FormRequest(last_url,formdata=formdata, callback=$callback$)'.replace('$callback$', callback)\n",
    "        script = script.replace('$method$', method_sc)\n",
    "        req_meta_sc = self._create_req_meta_(req_meta)\n",
    "        script = script.replace('$req_meta$', req_meta_sc)\n",
    "        return script\n",
    "\n",
    "    def _to_next_step(self,next_step_type,req_meta=['items'], method='get', callback='self.parse'):\n",
    "        # next_step_type:item or req\n",
    "        if next_step_type=='req':\n",
    "            script = '''\n",
    "            formdata={}\n",
    "            url=url_\n",
    "            req = $method$\n",
    "            req.meta['item'] = copy.deepcopy(item)\n",
    "            req.meta['page'] = page\n",
    "            req.meta['url_'] = copy.deepcopy(url_)\n",
    "            $req_meta$\n",
    "            yield req\n",
    "    '''\n",
    "            if method.lower() <> 'post':\n",
    "                method_sc = 'scrapy.Request(url, callback=$callback$)'.replace('$callback$', callback)\n",
    "            else:\n",
    "                method_sc = 'scrapy.FormRequest(url,formdata=formdata, callback=$callback$)'.replace('$callback$', callback)\n",
    "            script = script.replace('$method$', method_sc)\n",
    "            req_meta_sc = self._create_req_meta_(req_meta, 12)\n",
    "            script = script.replace('$req_meta$', req_meta_sc)\n",
    "        else:\n",
    "              script='''\n",
    "            items.append(dict(item))\n",
    "        req=scrapy.Request('https://www.baidu.com',callback=self.tomongodb,dont_filter=True)\n",
    "        req.meta['items']=copy.deepcopy(items)\n",
    "        yield req\n",
    "    def tomongodb(self,response):\n",
    "        items=copy.deepcopy(response.meta['items'])\n",
    "        # 写入数据库，通用\n",
    "        if items <> []:\n",
    "            def pre_process_item(item):\n",
    "                for key, value in item.iteritems():\n",
    "                    if isinstance(value, str) or isinstance(value, unicode):\n",
    "                        item[key] = value.replace('\\\\t', '').replace('\\\\r', '').replace('\\\\n', '').strip()\n",
    "                return item\n",
    "            items = map(pre_process_item, items)\n",
    "            # 修改数据库名、数据表名\n",
    "            client = MongoClient(MONGODB_IP, 27017)\n",
    "            # 修改数据库名、数据表名\n",
    "            db = client[MONGODB_DB]\n",
    "            db_key = db[MONGODB_TABLE]\n",
    "            info = db_key.insert_many(items)\n",
    "              '''\n",
    "        return script\n",
    "\n",
    "    def __call__(self,dirpath,mapCrawler,spiderName,items,db_settings):\n",
    "        self.dir=dirpath\n",
    "        self.projectName=mapCrawler\n",
    "        self.spiderName=spiderName\n",
    "        self.items=items\n",
    "        self.req_meta=['keyword']\n",
    "        \n",
    "        self.createProject()\n",
    "        self.settings(db_set[0],db_set[1],db_set[2])\n",
    "        self.createItems(self.items)\n",
    "        os.chdir('..\\\\')\n",
    "        os.chdir('..\\\\')\n",
    "        self.createSpider()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hai\\\\Desktop\\\\python\\\\AIscrapy\\\\out4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\hai\\\\Desktop\\\\python\\\\AIscrapy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建一个爬虫工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "items=['spider_name','date','app','city','brand','model','ad_content','top3_dealer','rank']\n",
    "db_set=['192.168.11.17','CrawlerData_other','YADI_DB_tiba']\n",
    "\n",
    "crawler=biliSpider()\n",
    "crawler('out','AutoAd','autohome_ad',items,db_set)\n",
    "crawler.StartReqByKw(req_method='get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加解析函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_keys=['spider_name','date','app','city','brand','model','ad_content','top3_dealer','rank']\n",
    "res_meta_keys=['page']\n",
    "req_meta_keys=[]\n",
    "crawler.ParseJson(name='parse_list_item',line_keys=line_keys,res_meta_keys=res_meta_keys,next_step_type='req',TP_method='get',req_meta_keys=req_meta_keys,callback='self.parse_item',method='get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加解析函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_keys=['content']\n",
    "res_meta_keys=['page']\n",
    "req_meta_keys=[]\n",
    "crawler.ParseHtml(name='parse_item',line_keys=line_keys,next_step_type='item',res_meta_keys=res_meta_keys,TP_method='get',req_meta_keys=req_meta_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
